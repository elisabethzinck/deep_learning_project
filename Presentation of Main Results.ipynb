{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation of Main Results: Dense Passage Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, paragraph_len = 128, remove_search_results = False):\n",
    "    \"\"\"\n",
    "    Pre-processes the data by selecting relevant columns and finding the \n",
    "    paragraph with the correct answer .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : datasets.Dataset\n",
    "        The raw wiki_qa data - maybe with feature 'search_results' removed for \n",
    "        more efficient storage.\n",
    "    paragraph_len : int, optional\n",
    "        Number of words in paragraph. The default is 128.\n",
    "    remove_search_results : bool, optional\n",
    "        Remove feature 'search_results' from the data. The default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        datasets.dataset\n",
    "    Pre processed dataset with columns \n",
    "    ['answer', 'paragraph', 'question', 'question_id'].\n",
    "    \"\"\"\n",
    "   \n",
    "    # Select appropiate columns\n",
    "    if remove_search_results:\n",
    "        data = data.map(remove_columns = ['search_results'])\n",
    "    out_data = data.map(lambda example: {'wiki_text': example['entity_pages']['wiki_context'], \n",
    "                                         'wiki_title': example['entity_pages']['title'],\n",
    "                                             'answer': example['answer']['normalized_value']}, \n",
    "                            remove_columns=['question_source', 'answer', 'entity_pages'])\n",
    "    \n",
    "    # Remove entries without wiki text\n",
    "    out_data = out_data.filter(lambda example: len(example['wiki_text']) > 0)\n",
    "    \n",
    "    # Get paragraph with the answer\n",
    "    out_data = out_data.map(lambda example: {\n",
    "        'paragraph': get_paragraph_with_answer(example, paragraph_len)\n",
    "        }, remove_columns = ['wiki_text', 'wiki_title'])\n",
    "    \n",
    "    out_data = out_data.map(lambda example: {\n",
    "        'question': append_Q_token(example)\n",
    "        })\n",
    "    return(out_data)\n",
    "\n",
    "def get_paragraph_with_answer(example, paragraph_len):\n",
    "    \"\"\"\n",
    "    Return paragraph of paragraph_len from example['wiki_text'] with highest \n",
    "    similary to question + answer\n",
    "    \"\"\"\n",
    "    paragraphs = get_all_paragraphs(example, paragraph_len)\n",
    "    \n",
    "    # joining title and text without the '[P]' token \n",
    "    paragraphs_joined = [paragraphs[0][0][4:] + ' ' + paragraph[1] for paragraph in paragraphs]\n",
    "    \n",
    "    # similarities\n",
    "    target = example['question'] + example['answer'] \n",
    "    sim = get_tfidf_similarity([target], paragraphs_joined) \n",
    "        \n",
    "    # finding most similar containing the answer\n",
    "    n_para = sim.size\n",
    "    \n",
    "    idxs = np.argsort(sim)[0][-n_para:]\n",
    "    for p in range(len(idxs)):\n",
    "        idx = idxs[-p]\n",
    "        if example['answer'] in paragraphs[idx][1]:\n",
    "            break \n",
    "    \n",
    "    answer_paragraph = paragraphs[idx]\n",
    "    \n",
    "    return answer_paragraph\n",
    "\n",
    "\n",
    "def get_all_paragraphs(example, paragraph_len):\n",
    "    \"\"\"\n",
    "    Splits all wiki_texts of example into paragraphs of paragraph_len\n",
    "    \"\"\"\n",
    "    n_texts = len(example['wiki_text'])\n",
    "    all_paragraphs = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(n_texts):\n",
    "        tokens = tokenizer.tokenize(example['wiki_text'][i].lower())\n",
    "        paragraphs = [tokens[i:(i+paragraph_len)] \\\n",
    "                      for i in range(0, len(tokens), paragraph_len)]\n",
    "        \n",
    "        #Old paragraph for concatenating title with paragraph\n",
    "        #paragraphs = [example['wiki_title'][i].lower() + \\\n",
    "        #              ' ' + ' '.join(paragraph) for paragraph in paragraphs]\n",
    "        \n",
    "        # Paragraph as list of title and text \n",
    "        paragraphs = [['[P] ' + example['wiki_title'][i].lower()] + [' '.join(paragraph)] for paragraph in paragraphs]\n",
    "        \n",
    "        all_paragraphs += paragraphs \n",
    "    return all_paragraphs\n",
    "    \n",
    "    \n",
    "def append_Q_token(example):\n",
    "    \"\"\"\n",
    "    Appends P or Q to the paragraph or Question\n",
    "    \"\"\"\n",
    "    return(['[Q]'] + [example['question']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
