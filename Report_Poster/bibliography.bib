@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v5},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {arXiv:1706.03762v5},
file = {:C$\backslash$:/Users/elisa/Documents/GitHub/deep{\_}learning{\_}project/Articles/Attention is all you need.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-December},
year = {2017}
}
@misc{Karpukhin2020,
abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
archivePrefix = {arXiv},
arxivId = {2004.04906},
author = {Karpukhin, Vladimir and OÄŸuz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
eprint = {2004.04906},
file = {:C$\backslash$:/Users/elisa/Documents/GitHub/deep{\_}learning{\_}project/Articles/Dense Passage Retrieval for Open-Domain Question Answering.pdf:pdf},
title = {{Dense Passage Retrieval for Open-Domain Question Answering}},
url = {http://arxiv.org/abs/2004.04906},
year = {2020}
}
@misc{,
file = {:C$\backslash$:/Users/elisa/Documents/GitHub/deep{\_}learning{\_}project/Articles/Reading Wikipedia to Answer Open-Domain Questions.pdf:pdf},
title = {{facebookresearch/DrQA: Reading Wikipedia to Answer Open-Domain Questions}},
url = {https://github.com/facebookresearch/DrQA}
}
@misc{,
file = {:C$\backslash$:/Users/elisa/Documents/GitHub/deep{\_}learning{\_}project/Articles/SQuAD100,000 Questions for Machine Comprehension of Text.pdf:pdf},
title = {{SQuAD100,000 Questions for Machine Comprehension of Text.pdf}}
}
@article{Joshi2017,
abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.},
archivePrefix = {arXiv},
arxivId = {1705.03551},
author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
doi = {10.18653/v1/P17-1147},
eprint = {1705.03551},
file = {:C$\backslash$:/Users/elisa/Documents/GitHub/deep{\_}learning{\_}project/Articles/TriviaQA A Large Scale Distantly Supervised Challenge Dataset.pdf:pdf},
isbn = {9781945626753},
journal = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
pages = {1601--1611},
title = {{TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension}},
volume = {1},
year = {2017}
}
@article{Meek2018,
author = {Meek, Wen-tau Yih Christopher},
file = {:C$\backslash$:/Users/elisa/Documents/GitHub/deep{\_}learning{\_}project/Articles/WIKIQA A Challenge Dataset for Open-Domain Question Answering.pdf:pdf},
number = {September 2015},
pages = {2013--2018},
title = {{W IKI QA : A Challenge Dataset for Open-Domain Question Answering}},
year = {2018}
}
