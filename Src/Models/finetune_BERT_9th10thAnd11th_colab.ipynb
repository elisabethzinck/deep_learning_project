{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_BERT_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRU4K5zWVemp"
      },
      "source": [
        "This is the SANDBOX version of finetuning which tunes the 9th, 10th and 11th layer\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6qtNFQyCIsv"
      },
      "source": [
        "# Start cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soayi6Ax2IvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78337da4-31c0-4b96-f966-d13fa9faf908"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIhfaJA87lU-"
      },
      "source": [
        "# Loading requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBaXjuKD7ohz",
        "outputId": "fa501512-a8c8-4c5e-e2af-31e64968587b"
      },
      "source": [
        "! pip install datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyBsg1w_7qyN",
        "outputId": "fae11483-bc94-4a3b-92b1-f500118b14fc"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROqy8ROq8zf5"
      },
      "source": [
        "# Import things to get data to work\n",
        "from datasets import load_from_disk\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vOZHVO25zRn"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L6q90R05g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5b6c12-8589-479b-a1a2-462fe78daa34"
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvO9AFoF9KGo"
      },
      "source": [
        "sys.path.append(os.path.join('/content/drive/My Drive/deep_learning_project'))\n",
        "train_path = 'drive/My Drive/deep_learning_project/train_small'\n",
        "val_path = 'drive/My Drive/deep_learning_project/validation_small'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww9b_fsX5nXx"
      },
      "source": [
        "train_data = load_from_disk(train_path)\n",
        "validation_data = load_from_disk(val_path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2EhY8a58yaq",
        "outputId": "8d24aeef-0974-4c5a-c5b3-3c1793dea66e"
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'answer': 'york', 'paragraph': ['[P] judi dench', 'born in dorset and later moved to dublin where he was raised he met dench s mother while he was studying medicine at trinity college dublin dench attended the mount school a quaker independent secondary school in york and became a quaker her brothers one of whom was actor jeffery dench were born in tyldesley lancashire her niece emma dench is a roman historian and professor previously at birkbeck university of london and currently at harvard university career in britain dench has developed a reputation as one of the greatest actresses of the post war period primarily through her work in theatre which has been her forte throughout her career she has more than once been named number one in polls for britain s best actor early years'], 'question': ['[Q]', 'Where in England was Dame Judi Dench born?'], 'question_id': 'tc_3'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4JOtDMX9tyq"
      },
      "source": [
        "# Preparing data for modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI0jM5Xl9_kn"
      },
      "source": [
        "from math import ceil\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BertModel, AdamW"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm_gD863_UBR"
      },
      "source": [
        "def get_loss(sim):\n",
        "    nll = -(torch.diagonal(sim) - torch.logsumexp(sim, dim = 1))\n",
        "    return sum(nll)# return negative loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJOd_09093vQ"
      },
      "source": [
        "# Define data parameters\n",
        "batch_size = 16\n",
        "\n",
        "# Train\n",
        "n_sample_train = 1024 #ceil(len(train_data)*0.02) \n",
        "n_batches_train = ceil(n_sample_train/batch_size)\n",
        "# Validation\n",
        "n_sample_validation = 128 #ceil(len(validation_data)*0.05)\n",
        "n_batches_validation = ceil(n_sample_validation/batch_size)\n",
        "\n",
        "\n",
        "# Define model parameters\n",
        "lr = 5e-5\n",
        "n_epochs = 4\n",
        "\n",
        "# Define printing parameters\n",
        "n_batch_print = 5 # Prints every (n_batch_print) during training"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYfy9sQoKS7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769a94b6-3ae8-4632-ffe3-987d1b584190"
      },
      "source": [
        "print(f\"Validation Samples: {n_sample_validation}\")\n",
        "print(f\"Training Samples: {n_sample_train}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Samples: 128\n",
            "Training Samples: 1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAJeVShU-JLO",
        "outputId": "be78eb27-ac49-425e-f9c8-fc846adb3649"
      },
      "source": [
        "# NB CHANGE THIS CELL WHEN THE DATA IS READY###############\n",
        "# Subset data\n",
        "train_data = train_data.select(range(n_sample_train))\n",
        "validation_data = validation_data.select(range(n_sample_validation))\n",
        "\n",
        "# Tokenize data\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', padding = True)\n",
        "train_data = train_data.map(lambda example: {\n",
        "    'Q_input_ids': tokenizer(example['question'],padding = 'longest')['input_ids'],\n",
        "    'Q_attention_mask': tokenizer(example['question'], padding = 'longest')['attention_mask'],\n",
        "    'Q_token_type_ids': tokenizer(example['question'], padding = 'longest')['token_type_ids'],\n",
        "    'P_input_ids': tokenizer(example['paragraph'],  padding = 'longest')['input_ids'],\n",
        "    'P_attention_mask': tokenizer(example['paragraph'],  padding = 'longest')['attention_mask'],\n",
        "    'P_token_type_ids': tokenizer(example['paragraph'],  padding = 'longest')['token_type_ids']},\n",
        "    batched = True, batch_size= batch_size)\n",
        "\n",
        "validation_data = validation_data.map(lambda example: {\n",
        "    'Q_input_ids': tokenizer(example['question'],  padding = 'longest')['input_ids'],\n",
        "    'Q_attention_mask': tokenizer(example['question'],  padding = 'longest')['attention_mask'],\n",
        "    'Q_token_type_ids': tokenizer(example['question'],  padding = 'longest')['token_type_ids'],\n",
        "    'P_input_ids': tokenizer(example['paragraph'],  padding = 'longest')['input_ids'],\n",
        "    'P_attention_mask': tokenizer(example['paragraph'], padding = 'longest')['attention_mask'],\n",
        "    'P_token_type_ids': tokenizer(example['paragraph'],  padding = 'longest')['token_type_ids']},\n",
        "    batched = True, batch_size= batch_size)\n",
        "\n",
        "#%% Change to pytorch format. \n",
        "train_data.set_format(type = 'torch', \n",
        "                        columns = ['Q_input_ids', 'Q_attention_mask', 'Q_token_type_ids',\n",
        "                                   'P_input_ids', 'P_attention_mask', 'P_token_type_ids'])\n",
        "\n",
        "validation_data.set_format(type = 'torch', \n",
        "                        columns = ['Q_input_ids', 'Q_attention_mask', 'Q_token_type_ids',\n",
        "                                   'P_input_ids', 'P_attention_mask', 'P_token_type_ids'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at drive/My Drive/deep_learning_project/train_small/cache-2d124ce2cfc6e061.arrow\n",
            "Loading cached processed dataset at drive/My Drive/deep_learning_project/validation_small/cache-8b88a68e0064c9cf.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSlh6z4VKLtN"
      },
      "source": [
        "# Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmRsdg1d-xrw"
      },
      "source": [
        "# Get pretrained model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Freezing all layers except the last ones\n",
        "l11 = [\"encoder.layer.11\" in name[0] for name in model.named_parameters()]\n",
        "l10 = [\"encoder.layer.10\" in name[0] for name in model.named_parameters()]\n",
        "l9 = [\"encoder.layer.9\" in name[0] for name in model.named_parameters()]\n",
        "layers_to_opt = list(map(any, zip(*[l9, l10, l11])))\n",
        "params = [name[0] for name in model.named_parameters()]\n",
        "param_to_be_optimized = [name for (layer, name) in zip(layers_to_opt, params) if layer]\n",
        "for name, param in model.named_parameters():\n",
        "  if not name in param_to_be_optimized:\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Move model to cuda to train there\n",
        "model.to(device)\n",
        "\n",
        "optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr = lr) # filter object works as a generator "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRAU6y9JMa__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853b474c-5082-4571-8cea-3ab23b3edc2a"
      },
      "source": [
        "# Print the weights that are being updated\n",
        "for name, param in model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder.layer.9.attention.self.query.weight\n",
            "encoder.layer.9.attention.self.query.bias\n",
            "encoder.layer.9.attention.self.key.weight\n",
            "encoder.layer.9.attention.self.key.bias\n",
            "encoder.layer.9.attention.self.value.weight\n",
            "encoder.layer.9.attention.self.value.bias\n",
            "encoder.layer.9.attention.output.dense.weight\n",
            "encoder.layer.9.attention.output.dense.bias\n",
            "encoder.layer.9.attention.output.LayerNorm.weight\n",
            "encoder.layer.9.attention.output.LayerNorm.bias\n",
            "encoder.layer.9.intermediate.dense.weight\n",
            "encoder.layer.9.intermediate.dense.bias\n",
            "encoder.layer.9.output.dense.weight\n",
            "encoder.layer.9.output.dense.bias\n",
            "encoder.layer.9.output.LayerNorm.weight\n",
            "encoder.layer.9.output.LayerNorm.bias\n",
            "encoder.layer.10.attention.self.query.weight\n",
            "encoder.layer.10.attention.self.query.bias\n",
            "encoder.layer.10.attention.self.key.weight\n",
            "encoder.layer.10.attention.self.key.bias\n",
            "encoder.layer.10.attention.self.value.weight\n",
            "encoder.layer.10.attention.self.value.bias\n",
            "encoder.layer.10.attention.output.dense.weight\n",
            "encoder.layer.10.attention.output.dense.bias\n",
            "encoder.layer.10.attention.output.LayerNorm.weight\n",
            "encoder.layer.10.attention.output.LayerNorm.bias\n",
            "encoder.layer.10.intermediate.dense.weight\n",
            "encoder.layer.10.intermediate.dense.bias\n",
            "encoder.layer.10.output.dense.weight\n",
            "encoder.layer.10.output.dense.bias\n",
            "encoder.layer.10.output.LayerNorm.weight\n",
            "encoder.layer.10.output.LayerNorm.bias\n",
            "encoder.layer.11.attention.self.query.weight\n",
            "encoder.layer.11.attention.self.query.bias\n",
            "encoder.layer.11.attention.self.key.weight\n",
            "encoder.layer.11.attention.self.key.bias\n",
            "encoder.layer.11.attention.self.value.weight\n",
            "encoder.layer.11.attention.self.value.bias\n",
            "encoder.layer.11.attention.output.dense.weight\n",
            "encoder.layer.11.attention.output.dense.bias\n",
            "encoder.layer.11.attention.output.LayerNorm.weight\n",
            "encoder.layer.11.attention.output.LayerNorm.bias\n",
            "encoder.layer.11.intermediate.dense.weight\n",
            "encoder.layer.11.intermediate.dense.bias\n",
            "encoder.layer.11.output.dense.weight\n",
            "encoder.layer.11.output.dense.bias\n",
            "encoder.layer.11.output.LayerNorm.weight\n",
            "encoder.layer.11.output.LayerNorm.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPZogvfK9vF2",
        "outputId": "bc8b56b7-b49b-4e30-9e3c-dddb179f89d9"
      },
      "source": [
        "# The big loop :D \n",
        "epoch_train_loss = [None]*n_batches_train\n",
        "epoch_validation_loss = [None]*n_batches_validation\n",
        "train_loss = [None]*n_epochs\n",
        "validation_loss = [None]*n_epochs\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    print(f'### EPOCH: {epoch+1}/{n_epochs} ###')\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "    trainloader = iter(trainloader)\n",
        "    validationloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
        "    validationloader = iter(validationloader)\n",
        "    \n",
        "    # TRAINING MODEL\n",
        "    model.train()\n",
        "    for i, batch in enumerate(trainloader):\n",
        "        if i % n_batch_print == 0:\n",
        "          print(f'batch {i+1}/{len(trainloader)}')\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        # Make forward pass for paragraphs\n",
        "        input_ids = batch['P_input_ids'].to(device)\n",
        "        attention_mask = batch['P_attention_mask'].to(device)\n",
        "        token_type_ids = batch['P_token_type_ids'].to(device)\n",
        "        P_encoded_layers = model(input_ids=input_ids, \n",
        "                          attention_mask=attention_mask, \n",
        "                          token_type_ids=token_type_ids, output_hidden_states = True)#[0][:, 0, :]\n",
        "        # concatenating hidden layers\n",
        "        P_cat = torch.cat(tuple([P_encoded_layers.hidden_states[i] for i in [-3, -2, -1]]), dim=-1)  \n",
        "        P_cat = P_cat[:, 0, :]\n",
        "        \n",
        "        # Make forward pass for questions\n",
        "        input_ids = batch['Q_input_ids'].to(device)\n",
        "        attention_mask = batch['Q_attention_mask'].to(device)\n",
        "        token_type_ids = batch['Q_token_type_ids'].to(device)\n",
        "        Q_encoded_layers = model(input_ids=input_ids, \n",
        "                          attention_mask=attention_mask, \n",
        "                          token_type_ids=token_type_ids, output_hidden_states = True)#[0][:, 0, :]\n",
        "        # concatenating hidden layers\n",
        "        Q_cat = torch.cat(tuple([Q_encoded_layers.hidden_states[i] for i in [-3, -2, -1]]), dim=-1)  \n",
        "        Q_cat = Q_cat[:, 0, :]\n",
        "\n",
        "        # Calculate similarity matrix\n",
        "        sim_matrix = torch.matmul(Q_cat, P_cat.T)\n",
        "\n",
        "        # Get loss\n",
        "        loss = get_loss(sim_matrix)\n",
        "        #loss.requires_grad = True\n",
        "\n",
        "        # Update weights\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        # Save loss\n",
        "        epoch_train_loss[i] = loss.item()\n",
        "\n",
        "    # VALIDATING MODEL\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(validationloader):\n",
        "        \n",
        "        # (get extra observation in training set)\n",
        "\n",
        "        # Make forward pass for paragraphs\n",
        "        input_ids = batch['P_input_ids'].to(device)\n",
        "        attention_mask = batch['P_attention_mask'].to(device)\n",
        "        token_type_ids = batch['P_token_type_ids'].to(device)\n",
        "        P_encoded_layers = model(input_ids=input_ids, \n",
        "                          attention_mask=attention_mask, \n",
        "                          token_type_ids=token_type_ids, output_hidden_states = True)#[0][:, 0, :]\n",
        "        \n",
        "        # concatenating hidden layers\n",
        "        P_cat = torch.cat(tuple([P_encoded_layers.hidden_states[i] for i in [-3, -2, -1]]), dim=-1)  \n",
        "        P_cat = P_cat[:, 0, :]\n",
        "\n",
        "        # Make forward pass for questions\n",
        "        input_ids = batch['Q_input_ids'].to(device)\n",
        "        attention_mask = batch['Q_attention_mask'].to(device)\n",
        "        token_type_ids = batch['Q_token_type_ids'].to(device)\n",
        "        Q_encoded_layers = model(input_ids=input_ids, \n",
        "                          attention_mask=attention_mask, \n",
        "                          token_type_ids=token_type_ids, output_hidden_states = True)#[0][:, 0, :]\n",
        "        \n",
        "        # concatenating hidden layers\n",
        "        Q_cat = torch.cat(tuple([Q_encoded_layers.hidden_states[i] for i in [-3, -2, -1]]), dim=-1)  \n",
        "        Q_cat = Q_cat[:, 0, :]\n",
        "\n",
        "        # Calculate similarity matrix\n",
        "        sim_matrix = torch.matmul(Q_cat, P_cat.T)\n",
        "\n",
        "        # Get loss\n",
        "        loss = get_loss(sim_matrix)\n",
        "        epoch_validation_loss[i] = loss.item()\n",
        "        \n",
        "    \n",
        "    train_loss[epoch] = sum(epoch_train_loss)/len(epoch_train_loss)\n",
        "    validation_loss[epoch] = sum(epoch_validation_loss)/len(epoch_validation_loss)\n",
        "    \n",
        "    print(f'train loss: {train_loss[epoch]:.2f}')\n",
        "    print(f'validation loss: {validation_loss[epoch]:.2f}')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### EPOCH: 1/4 ###\n",
            "batch 1/16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.tensor(x, **format_kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch 6/16\n",
            "batch 11/16\n",
            "batch 16/16\n",
            "train loss: 511.45\n",
            "validation loss: 77.11\n",
            "### EPOCH: 2/4 ###\n",
            "batch 1/16\n",
            "batch 6/16\n",
            "batch 11/16\n",
            "batch 16/16\n",
            "train loss: 157.61\n",
            "validation loss: 45.04\n",
            "### EPOCH: 3/4 ###\n",
            "batch 1/16\n",
            "batch 6/16\n",
            "batch 11/16\n",
            "batch 16/16\n",
            "train loss: 74.44\n",
            "validation loss: 35.38\n",
            "### EPOCH: 4/4 ###\n",
            "batch 1/16\n",
            "batch 6/16\n",
            "batch 11/16\n",
            "batch 16/16\n",
            "train loss: 49.21\n",
            "validation loss: 31.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnB-hrrIOe-S"
      },
      "source": [
        "#model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvSCo3xN4INj"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3i1oLjBWL9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd54a118-bbe4-4c88-c5ff-46b5f7ea0491"
      },
      "source": [
        "print(validation_loss)\n",
        "print(train_loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[77.10686492919922, 45.03623962402344, 35.380069732666016, 31.785701751708984]\n",
            "[511.4469404220581, 157.60506439208984, 74.43839406967163, 49.209158182144165]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM095XYxWEpe"
      },
      "source": [
        "#%% Saving model\n",
        "#model_path = 'drive/My Drive/BERT_Model/8_percent/'\n",
        "#model.save_pretrained(model_path)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OOaGE4RAaaA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4870e22a-9cba-4489-84b6-71595a02af60"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(range(1, n_epochs+1), train_loss, label = 'Train loss')\n",
        "plt.plot(range(1, n_epochs+1), validation_loss, label = 'Validation loss')\n",
        "plt.title('Finetuning BERT Base Model')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()\n",
        "#fig.savefig(\"drive/My Drive/deep_learning_project/figures/BERTbaseFinetuning_pooled_output.pdf\", bbox_inches='tight')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcJgm4bKCQ9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec3a12b-c424-4de2-f99b-8a78a2b8b85a"
      },
      "source": [
        "validation_loss\n",
        "train_loss"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[511.4469404220581, 157.60506439208984, 74.43839406967163, 49.209158182144165]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt6-DU072GYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c656f72-3442-4a48-cc9d-5dd16dac49b9"
      },
      "source": [
        "print(validation_loss)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[77.10686492919922, 45.03623962402344, 35.380069732666016, 31.785701751708984]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC4ui8WM2HRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580c7bab-c94e-40e3-b4ca-d989e559fc20"
      },
      "source": [
        "print(train_loss)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[511.4469404220581, 157.60506439208984, 74.43839406967163, 49.209158182144165]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz23rcyLh_TQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b5b441-62a8-412d-aff4-9dd0afd40f8e"
      },
      "source": [
        "# Andreas' code he ask us to run on slack \n",
        "from prettytable import PrettyTable\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------+------------+\n",
            "|                      Modules                       | Parameters |\n",
            "+----------------------------------------------------+------------+\n",
            "|    encoder.layer.9.attention.self.query.weight     |   589824   |\n",
            "|     encoder.layer.9.attention.self.query.bias      |    768     |\n",
            "|     encoder.layer.9.attention.self.key.weight      |   589824   |\n",
            "|      encoder.layer.9.attention.self.key.bias       |    768     |\n",
            "|    encoder.layer.9.attention.self.value.weight     |   589824   |\n",
            "|     encoder.layer.9.attention.self.value.bias      |    768     |\n",
            "|   encoder.layer.9.attention.output.dense.weight    |   589824   |\n",
            "|    encoder.layer.9.attention.output.dense.bias     |    768     |\n",
            "| encoder.layer.9.attention.output.LayerNorm.weight  |    768     |\n",
            "|  encoder.layer.9.attention.output.LayerNorm.bias   |    768     |\n",
            "|     encoder.layer.9.intermediate.dense.weight      |  2359296   |\n",
            "|      encoder.layer.9.intermediate.dense.bias       |    3072    |\n",
            "|        encoder.layer.9.output.dense.weight         |  2359296   |\n",
            "|         encoder.layer.9.output.dense.bias          |    768     |\n",
            "|      encoder.layer.9.output.LayerNorm.weight       |    768     |\n",
            "|       encoder.layer.9.output.LayerNorm.bias        |    768     |\n",
            "|    encoder.layer.10.attention.self.query.weight    |   589824   |\n",
            "|     encoder.layer.10.attention.self.query.bias     |    768     |\n",
            "|     encoder.layer.10.attention.self.key.weight     |   589824   |\n",
            "|      encoder.layer.10.attention.self.key.bias      |    768     |\n",
            "|    encoder.layer.10.attention.self.value.weight    |   589824   |\n",
            "|     encoder.layer.10.attention.self.value.bias     |    768     |\n",
            "|   encoder.layer.10.attention.output.dense.weight   |   589824   |\n",
            "|    encoder.layer.10.attention.output.dense.bias    |    768     |\n",
            "| encoder.layer.10.attention.output.LayerNorm.weight |    768     |\n",
            "|  encoder.layer.10.attention.output.LayerNorm.bias  |    768     |\n",
            "|     encoder.layer.10.intermediate.dense.weight     |  2359296   |\n",
            "|      encoder.layer.10.intermediate.dense.bias      |    3072    |\n",
            "|        encoder.layer.10.output.dense.weight        |  2359296   |\n",
            "|         encoder.layer.10.output.dense.bias         |    768     |\n",
            "|      encoder.layer.10.output.LayerNorm.weight      |    768     |\n",
            "|       encoder.layer.10.output.LayerNorm.bias       |    768     |\n",
            "|    encoder.layer.11.attention.self.query.weight    |   589824   |\n",
            "|     encoder.layer.11.attention.self.query.bias     |    768     |\n",
            "|     encoder.layer.11.attention.self.key.weight     |   589824   |\n",
            "|      encoder.layer.11.attention.self.key.bias      |    768     |\n",
            "|    encoder.layer.11.attention.self.value.weight    |   589824   |\n",
            "|     encoder.layer.11.attention.self.value.bias     |    768     |\n",
            "|   encoder.layer.11.attention.output.dense.weight   |   589824   |\n",
            "|    encoder.layer.11.attention.output.dense.bias    |    768     |\n",
            "| encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n",
            "|  encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n",
            "|     encoder.layer.11.intermediate.dense.weight     |  2359296   |\n",
            "|      encoder.layer.11.intermediate.dense.bias      |    3072    |\n",
            "|        encoder.layer.11.output.dense.weight        |  2359296   |\n",
            "|         encoder.layer.11.output.dense.bias         |    768     |\n",
            "|      encoder.layer.11.output.LayerNorm.weight      |    768     |\n",
            "|       encoder.layer.11.output.LayerNorm.bias       |    768     |\n",
            "+----------------------------------------------------+------------+\n",
            "Total Trainable Params: 21263616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21263616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CrjtoTY1pCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c5fb5d-b4bd-4122-d776-0430e68db40d"
      },
      "source": [
        "tmp.last_hidden_state.size()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 283, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35-EOKgVK1FP",
        "outputId": "9fa029b7-d212-493c-d450-0c573c32f292"
      },
      "source": [
        "model"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}